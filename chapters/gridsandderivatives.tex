\chapter[Grids and Numerical Derivatives]{Grids and Numerical Derivatives}
\label{ch:grids}
%\addcontentsline{toc}{chapter}{Grids and Derivatives}

When we solved differential equations in Physics 295 we were usually
moving something forward in time, so you may have the impression that
differential equations always ``flow.'' This is not true. If we solve
a spatial differential equation, for instance, like the one that
gives the shape of a chain draped between two posts, the solution
just sits in space; nothing flows. Instead, we choose a small spatial
step size (think of each individual link in the chain) and we seek to
find the correct shape by somehow finding the height of the chain at
each link.

In this course we will be solving partial differential equations,
which usually means that the desired solution is a function of both
space $x$, which just sits, and time $t$, which flows.
\index{Spatial grids} And when we solve problems like this we will
be using {\it spatial grids}, to represent the $x$-part that
doesn't flow. You have already used grids in Python to do simple
jobs like plotting functions and doing integrals numerically.
Before we proceed to solving partial differential equations, let's
spend some time getting comfortable working with spatial grids.

\marginfig[-1in]{chapters/f01Grids}{\label{f01Grids}Three common spatial grids}

\labsection{Spatial grids}

Figure~\ref{f01Grids} shows a graphical representation of three
types of spatial grids for the region $0 \le x \le L$.  We divide
this region into spatial \emph{cells} (the spaces between vertical
lines) and functions are evaluated at $N$ discrete \emph{grid
points} (the dots). In a \emph{cell-edge} grid,\index{Cell-edge
grid}\index{Grids!cell-edge} the grid points are located at the
edge of the cell.  In a \emph{cell-center} grid,\index{Cell-center
grid}\index{Grids!cell-center} the points are located in the middle
of the cell.  Another useful grid is a cell-center grid with {\it
ghost points}.\index{Ghost points} The ghost points (unfilled dots)
are extra grid points on either side of the interval of interest
and are useful when we need to consider the derivatives at the edge
of a grid.

\begin{enumerate}
\prob \label{P:1.1}
\begin{enumerate}
\subprob \label{P:1.1a} Make a Python script that creates a
    cell-edge spatial grid in the variable {\tt x} as
    follows: \marginfig{chapters/f01p1a}{Plot from \ref{P:1.1a}}
\begin{Verbatim}
from numpy import arange  # Import the needed function
N=100             % the number of grid points
a=0
b=pi          % the left and right bounds
h=(b-a)/(N-1)     % calculate the step size
x=arange(a,b,h)           % build the grid
\end{Verbatim}
    Plot the function $y(x) = \sin(x) \sinh(x)$ on this grid.
    Explain the relationship between the number of cells and
    the number of grid points in a cell-edge grid and why you
    divide by {\tt (N-1)} when calculating {\tt h}. Then
    verify that the number of points in this $x$-grid is $N$
    (using Python's {\tt len} command).

\subprob \label{P:1.1b} Explain the relationship between the
    number of cells and the number of grid points in a
    cell-center grid and decide how you should modify the
    line that calculates {\tt h} in (a) to get the correct
    spacing for a cell-center grid.

    \marginfig{chapters/f01p1b}{Plot from \ref{P:1.1b}}

    Now write a script like the one in part (a) to build a cell-center
    grid over the interval $0 \le x \le 2$ with $N=5000$. Evaluate the
    function $f(x)=\cos{x}$ on this grid and plot this function. Then
    estimate the area under the curve by summing the products of the
    centered function values $f_j$ with the widths of the cells $h$
    like this (midpoint integration rule):
\begin{Verbatim}
sum(f)*h;
\end{Verbatim}

    Verify that this result is quite close to the exact
    answer obtained by integration:
    \[
        A=\int_0^2 \cos{x} ~dx.
    \]

\subprob Build a cell-center grid with ghost points over the
    interval $0 \le x \le \pi/2$ with 500~cells (502 grid
    points), and evaluate the function $f(x)=\sin{x}$ on this
    grid.  Now look carefully at the function values at the
    first two grid points and at the last two grid points.
    The function $\sin{x}$ has the property that $f(0)=0$ and
    $f'(\pi/2)=0$. The cell-center grid doesn't have points
    at the ends of the interval, so these boundary conditions
    on the function need to be enforced using more than one
    point. Explain how the ghost points can be used in
    connection with interior points to specify both
    function-value boundary conditions and derivative-value
    boundary conditions.
\end{enumerate}
\end{enumerate}

%Python has a convenient command {\tt linspace} for building one
%dimensional grids.  The syntax for building a grid is
%\begin{Verbatim}
%x = linspace(a,b,N);
%\end{Verbatim}
%where {\tt a} is the $x$ position of the first point in the grid,
%{\tt b} is the $x$-position of the last point in the grid, and {\tt
%N} is the number of grid points.  This method doesn't give you the
%grid spacing back, but you can always calculate it by subtraction:
%\begin{Verbatim}
%h = x(2) - x(1);
%\end{Verbatim}
%Depending on what you choose for {\tt a} and {\tt b}, {\tt linspace}
%can give you either cell-edge or cell-center grids.

\labsection{Interpolation and extrapolation}
\index{Interpolation} \index{Extrapolation}

Grids only represent functions at discrete points, and there will be
times when we want to find good values of a function {\it between}
grid points (interpolation) or \emph{beyond} the last grid point
(extrapolation). We will use interpolation and extrapolation
techniques fairly often during this course, so let's review these
ideas.

\marginfig{chapters/f01Linear}{The line defined by two points can be used to
interpolate between the points and extrapolate beyond the points.}

The simplest way to estimate these values is to use the fact that two
points define a straight line. For example, suppose that we have
function values $(x_1,y_1)$ and $(x_2,y_2)$. The formula for a
straight line that passes through these two points is
\begin{equation} \label{eq:linear}
    y-y_1 = \frac{ (y_2-y_1) }{ (x_2-x_1) } (x-x_1)
\end{equation}
Once this line has been established it provides an approximation to
the true function $y(x)$ that is pretty good in the neighborhood of
the two data points. To linearly interpolate or extrapolate we simply
evaluate Eq.~(\ref{eq:linear}) at $x$ values between or beyond $x_1$
and $x_2$.

\begin{enumerate}
\prob \label{P:1.3} Use Eq.~(\ref{eq:linear}) to do the following
    special cases:

\begin{enumerate}
\subprob Find an approximate value for $y(x)$ halfway between
    the two points $x_1$ and $x_2$. Does your answer make
    sense?

\subprob Find an approximate value for $y(x)$ 3/4 of the way
    from $x_1$ to $x_2$. Do you see a pattern?

\subprob If the spacing between grid points is $h$ (i.e.
    $x_2-x_1=h$), show that the linear extrapolation formula
    for $y(x_2+h)$ is
    \begin{equation}\label{eq:linExtrap}
        y(x_2+h) = 2 y_2 - y_1
    \end{equation}
    This provides a convenient way to estimate the function
    value one grid step beyond the last grid point.  Also
    show that
    \begin{equation}\label{eq:linExtraphalf}
        y(x_2+h/2) = 3 y_2 / 2 - y_1 / 2 .
    \end{equation}
    We will use both of these formulas during the course.
\end{enumerate}
\end{enumerate}

\marginfig{chapters/f01Quadratic}{Three points define a parabola that can be
used to interpolate between the points and extrapolate beyond the
points.}

A fancier technique for finding values between and beyond grid points
is to use a parabola instead of a line. It takes three data points to
define a parabola, so we need to start with the function values
$(x_1,y_1)$, $(x_2,y_2)$, and $(x_3,y_3)$. The general formula for a
parabola is
\begin{equation}\label{eq:Parabola}
    y=a + bx + cx^2
\end{equation}
where the coefficients $a$, $b,$ and $c$ need to be chosen so that
the parabola passes through our three data points. To determine these
constants, you set up three equations that force the parabola to
match the data points, like this:
\begin{equation}\label{eq:ParabolaSet}
    y_j = a + bx_j + cx_j^2
\end{equation}
with $j=1,2,3$, and then solve for $a$, $b$, and $c$.

\begin{enumerate}
\prob \label{P:1.4} Use Eq.~(\ref{eq:ParabolaSet}) to create a
    set of three equations in Mathematica. For simplicity, assume
    that the points are on an evenly-spaced grid and set
    $x_2=x_1+h$ and $x_3=x_1+2h$.  Solve this set of equations to
    obtain some messy formulas for $a$, $b$, and $c$ that involve
    $x_1$ and $h$. Then use these formulas to solve the following
    problems:

\begin{enumerate}

\subprob Estimate $y(x)$ half way between $x_1$ and $x_2$,
    and then again halfway between $x_2$ and $x_3$. Do you
    see a pattern? (You will need to simplify the answer that
    Mathematica spits out to see the pattern.)

\subprob Show that the quadratic extrapolation formula for
    $y(x_3+h)$ (i.e. the value one grid point beyond $x_3$)
    is
    \begin{equation}\label{eq:quadExtrap}
        y(x_3+h) = y_1 - 3 y_2 + 3 y_3
    \end{equation}
    Also find the formula for $y(x_3+h/2)$.
\end{enumerate}
\end{enumerate}


\labsection{Derivatives on grids}
\index{Derivatives, first and second}
\index{Forward difference formula}

This is a course on partial differential equations, so we will
frequently need to calculate derivatives on our grids. In your
introductory calculus book, the derivative was probably introduced
using the {\it forward difference} formula
\begin{equation}\label{eq:forwarddiff}
     f'(x) \approx \frac{f(x+h)-f(x)}{h} .
\end{equation}
The word ``forward'' refers to the way this formula reaches forward
from $x$ to $x+h$ to calculate the slope. The exact derivative
represented by Eq.~(\ref{eq:forwarddiff}) in the limit that $h$
approaches zero.  However, we can't make $h$ arbitrarily small when
we represent a function on a grid because (i) the number of cells
needed to represent a region of space becomes infinite as $h$ goes to
zero; and (ii) computers represent numbers with a finite number of
significant digits so the subtraction in the numerator of
Eq.~(\ref{eq:forwarddiff}) loses accuracy when the two function
values are very close. But given these limitation we want to be as
accurate as possible, so we want to use the best derivative formulas
available. The forward difference formula isn't one of them.

\marginfig{chapters/f01FiniteDifference}{The forward and centered difference
formulas both approximate the derivative as the slope of a line
connecting two points. The centered difference formula gives a more
accurate approximation because it uses points before and after the
point where the derivative is being estimated. (The true derivative
is the slope of the dotted tangent line).}

The best first derivative formula that uses only two function values
is usually the {\it centered difference} formula: \index{Centered
difference formula}
\begin{equation}\label{eq:CenteredDiff}
    f'(x) \approx \frac{f(x+h)-f(x-h)}{2h} .
\end{equation}
It is called ``centered'' because the point $x$ at which we want the
slope is centered between the places where the function is evaluated.
The corresponding centered second derivative formula is \index{Second
derivative}
\begin{equation}\label{eq:seconderivative}
    f''(x) \approx {f(x+h)-2 f(x)+f(x-h) \over h^2}
\end{equation}
You will derive both of these formulas a little later, but for now we
just want you to understand how to use them.

Python's colon operator provides a compact way to evaluate
Eqs.~(\ref{eq:CenteredDiff}) and (\ref{eq:seconderivative}) on a
grid.  If the function we want to take the derivative of is stored in
an array {\tt fp}, we can calculate the centered first derivative like this:
\begin{Verbatim}
fp(2:N-1)=(f(3:N)-f(1:N-2))/(2*h);
\end{Verbatim}
and second derivative formulas at each interior grid point like this:
\begin{Verbatim}
fpp(2:N-1)=(f(3:N)-2*f(2:N-1)+f(1:N-2))/h^2;
\end{Verbatim}
The variable {\tt h} is the spacing between grid points and {\tt N}
is the number of grid points. (Both variables need to be set before
the derivative code above will work.) Study this code until you are
convinced that it represents Eqs.~(\ref{eq:CenteredDiff}) and
(\ref{eq:seconderivative}) correctly. If this code looks mysterious
to you, you may need to review how the colon operator works in the
330~manual \emph{Introduction to Python}.

The derivative at the first and last points on the grid can't be
calculated using Eqs.~(\ref{eq:CenteredDiff}) and
(\ref{eq:seconderivative}) since there are not grid points on both
sides of the endpoints. About the best we can do is to extrapolate
the interior values of the two derivatives to the end
points.\index{Extrapolation} If we use linear extrapolation
\index{Linear extrapolation} then we just need two nearby points, and
the formulas for the derivatives at the end points are found using
Eq.~(\ref{eq:linExtrap}):
\begin{Verbatim}
fp(1)=2*fp(2)-fp(3);
fp(N)=2*fp(N-1)-fp(N-2);
fpp(1)=2*fpp(2)-fpp(3);
fpp(N)=2*fpp(N-1)-fpp(N-2);
\end{Verbatim}
If we extrapolate using parabolas (quadratic extrapolation),
\index{Quadratic extrapolation}  we need to use three nearby points
as specified by Eq.~(\ref{eq:quadExtrap}):
\begin{Verbatim}
fp(1)=3*fp(2)-3*fp(3)+fp(4);
fp(N)=3*fp(N-1)-3*fp(N-2)+fp(N-3);
fpp(1)=3*fpp(2)-3*fpp(3)+fpp(4);
fpp(N)=3*fpp(N-1)-3*fpp(N-2)+fpp(N-3);
\end{Verbatim}

\begin{enumerate}
\prob \label{P:1.Deriv} \marginfig{chapters/f01p4}{Plots from
\ref{P:1.Deriv}}

    Create a cell-edge grid with $N=100$ on the interval $0 \le x
    \le 5$. Load $f(x)$ with the Bessel function $J_0(x)$ and
    numerically differentiate it to obtain $f'(x)$ and $f''(x)$.
    Use both linear and quadratic extrapolation to calculate the
    derivative at the endpoints. Compare both extrapolation
    methods to the exact derivatives and check to see how much
    better the quadratic extrapolation works. Then make overlaid
    plots of the numerical derivatives with the exact
    derivatives:
    \[
        f'(x) = -J_1(x)
    \]
    \[
        f''(x) = \frac{1}{2} \left( -J_0(x) + J_2(x) \right)
    \]
\end{enumerate}

\labsection{Errors in the approximate derivative formulas}

We'll conclude this lab with a look at where the approximate
derivative formulas come from and at the types of the errors that pop
up when using them. The starting point is Taylor's expansion of the
function $f$ a small distance $h$ away from the point $x$
\index{Taylor expansion}
\begin{equation}\label{eq:Taylor}
    f(x+h) = f(x) + f'(x) h + {1 \over 2} f''(x) h^2 +
    ~\cdots~+ f^{(n)}(x) {h^n \over n!}
    +~\cdots
\end{equation}
Let's use this series to understand the forward difference
approximation to $f'(x)$. If we apply the Taylor expansion to the
$f(x+h)$ term in Eq.~(\ref{eq:forwarddiff}), we get
\begin{eqnarray}\label{eq:ForExpand}
    \frac{f(x+h)-f(x)}{h}
    = \frac{\left[f(x)+f'(x)h + \frac{1}{2} f''(x) h^2 + \cdots \right]-f(x)}{h}
\end{eqnarray}
The higher order terms in the expansion (represented by the dots) are
smaller than the $f''$ term because they are all multiplied by higher
powers of $h$ (which we assume to be small). If we neglect these
higher order terms, we can solve Eq.~(\ref{eq:ForExpand}) for the
exact derivative $f'(x)$ to find
\begin{equation} \label{eq:ForwardError}
    f'(x) \approx {f(x+h)-f(x) \over h} - {h \over 2} f''(x)
\end{equation}
From Eq.~\eqref{eq:ForwardError} we see that the forward difference
does indeed give the first derivative back, but it carries an error
term which is proportional to $h$. But, of course, if $h$ is small
enough then the contribution from the term containing $f''(x)$ will
be too small to matter and we will have a good approximation to
$f'(x)$.

Now let's perform the same analysis on the centered difference
formula to see why it is better. Using the Taylor expansion for both
$f(x+h)$ and $f(x-h)$ in Eq.~(\ref{eq:CenteredDiff}) yields
\begin{eqnarray}\label{eq:cenExpand}
    \frac{f(x+h)-f(x-h)}{2 h} &=&
    \frac{\left[f(x)+f'(x)h + f''(x) \frac{h^2}{2}+f'''(x) \frac{h^3}{6} + \cdots \right]}{2 h}
    \\
    & & \qquad \qquad -
    \frac{\left[ f(x)-f'(x)h + f''(x) \frac{h^2}{2}-f'''(x)\frac{h^3}{6} + \cdots \right] }{2 h}
    \nonumber
\end{eqnarray}
If we again neglect the higher-order terms, we can solve
Eq.~(\ref{eq:cenExpand}) for the exact derivative $f'(x)$. This time,
the $f''$ terms exactly cancel to give
\begin{equation} \label{eq:CenteredError}
    f'(x) \approx {f(x+h)-f(x-h) \over 2 h} - {h^2 \over 6} f'''(x)
\end{equation}
Notice that for this approximate formula the error term is much smaller, only
of order $h^2$. To get a feel why this is so much better, imagine decreasing
$h$ in both the forward and centered difference formulas by a factor of 10.
The forward difference error will decrease by a factor of 10, but the
centered difference error will decrease by a factor of 100. This is the
reason we try to use centered formulas whenever possible in this course.

\begin{enumerate}
\prob \label{P:1.fppDeriv}

\begin{enumerate}
\subprob Let's find the second derivative formula using an
    approach similar to what we did for the first derivative.
    In Mathematica, write out the Taylor's expansion for
    $f(x+h)$ using Eq.~\eqref{eq:Taylor}, but change the
    derivatives to variables that Mathematica can do algebra
    with, like this:
\begin{Verbatim}
fplus=f + fp*h + fp2*h^2/2 + fp3*h^3/6 + fp4*h^4/24
\end{Verbatim}
    where {\tt fp} stands for $f'$, {\tt fp2} stands for
    $f''$, etc. Make a similar equation called {\tt eqminus}
    for $f(x-h)$ that contains the same derivative variables
    {\tt fp}, {\tt fpp}, etc. Now solve these two equations
    for the first derivative {\tt fp} and the second
    derivative {\tt fpp}. Verify that the first derivative
    formula matches Eq.~\eqref{eq:CenteredError}, including
    the error term, and that the second derivative formula
    matches Eq.~\eqref{eq:seconderivative}, but now with the
    appropriate error term. What order is the error in terms
    of the step size $h$?

%\footnote{Notice we have
%assumed that the points on our grid are equally spaced
%by $h$ when deriving the finite difference expressions.
%This assumption is not mandatory, but it makes life
%easier so we always use equally spaced grids in this
%course. If you find yourself in a situation where you
%need to use an unequally spaced rectangular grid, you
%can get appropriate derivative formulas using the
%techniques of problem~\eqref{P:1.fppDeriv}.  Just
%expand using something like $f(x+a)$ and $f(x-b)$.
%You'll get some more complicated expressions that
%reduce to what we found when $a=b$.}
%
%\subprob Examine your solution from (b) and write down
%the approximate formulas for the first and second
%derivatives. Then examine them further to show that the
%error in the first derivative formula using these three
%points is of second order in the step sizes $p$ and $m$
%(note that $pm$ is also second order) but that the
%second derivative formula only has a second order error
%if we set $p=m$, in which case we find
%Eq.~(\ref{eq:seconderivative}).

\subprob \label{P:1.5b} \textbf{Extra Credit:} (Finish the
    rest of the lab before doing this problem.)

    Now let's look for a reasonable approximation for the
    third derivative. Suppose you have function values
    $f(x-3h/2)$, $f(x-h/2)$, $f(x+h/2)$, and $f(x+3h/2)$.
    Using Mathematica and the procedure in (a), write down
    four ``algebraic Taylor's'' series up to the fifth
    derivative for the function at these four points. Then
    solve this system of four equations to find expressions
    for $f(x)$, $f'(x)$, $f''(x)$, and $f'''(x)$ (i.e. solve
    the system for the variables {\tt f}, {\tt fp}, {\tt
    fp2}, and {\tt fp3} if you use the same notation as (a)).
    Focus on the expression for the third derivative.  You
    should find the approximate formula
    \begin{equation}\label{eq:thirdDeriv}
        f'''(x) \approx
        \frac{f(x+3h/2) - 3 f(x+h/2) + 3 f(x-h/2) - f(x-3h/2)}{h^3}
    \end{equation}
    along with an error term on the order of $h^2$.  This
    expression will be useful when we need to approximate a
    third derivative on a grid in Lab~\ref{Lab:13}.
\end{enumerate}

\marginfig{chapters/f01p6}{Error in the forward and centered difference
approximations to the first derivative and the centered
difference formula for the second derivative as a function of
$h$.  The function is $e^x$ and the approximations are
evaluated for $x=0$.}

\prob \label{P:DerivativeRoundoff} Use Python (or a calculator)
    to compute the forward and centered difference formulas for
    the function $f(x)=e^x$ at $x=0$ with $h=0.1,~0.01,~0.001$.
    Also calculate the centered second derivative formula for
    these values of $h$. Verify that the error estimates in
    Eqs.~(\ref{eq:ForwardError}) and (\ref{eq:CenteredError})
    agree with the numerical testing.

    Note that at $x=0$ the exact values of both $f'$ and $f''$
    are equal to 1, so just subtract 1 from your numerical result
    to find the error.
\end{enumerate}

In problem~\ref{P:DerivativeRoundoff}, you should have found that
$h=0.001$ in the centered-difference formula gives a better
approximation than $h=0.01$.  These errors are due to the finite
grid spacing $h$, which might entice you to try to keep making $h$
smaller and smaller to achieve any accuracy you want. This doesn't
work. Figure~\ref{f01p6} shows a plot of the error you calculated
in problem~\ref{P:DerivativeRoundoff} as $h$ continues to decrease
(note the log scales). For the larger values of $h$, the errors
track well with the predictions made by the Taylor's series
analysis. However, when $h$ becomes too small, the error starts to
increase. Finally (at about $h=10^{-16}$, and sooner for the second
derivative) the finite difference formulas have no accuracy at
all---the error is the same order as the derivative.

The reason for this behavior is that numbers in computers are
represented with a finite number of significant digits.  Most
computational languages (including Python) use a representation that
has 15-digit accuracy. This is normally plenty of precision, but look
what happens in a subtraction problem where the two numbers are
nearly the same:
\begin{equation}
    \begin{array}{ll}
    & 7.38905699669556 \\
    - & 7.38905699191745 \\
    \hline
    &  0.00000000477811
    \end{array}
\end{equation}
Notice that our nice 15-digit accuracy has disappeared, leaving
behind only 6 significant figures. This problem occurs in
calculations with real numbers on all digital computers, and is
called {\it roundoff}. \index{Roundoff} You can see this effect by
experimenting with the Python command
\begin{Verbatim}
h=1e-17; (1+h); ans-1
\end{Verbatim}
for different values of $h$ and noting that you don't always get
$h$ back. Also notice in Fig.~\ref{f01p6} that this problem is
worse for the second derivative formula than it is for the first
derivative formula. The lesson here is that it is impossible to
achieve arbitrarily high accuracy by using arbitrarily tiny values
of $h$. In a problem with a size of about $L$ it doesn't do any
good to use values of $h$ any smaller than about $0.0001 L$.

%\begin{enumerate}
%\prob \label{P:1.Round}
%Experiment with the Python command:
%\begin{Verbatim}
%h=1e-17; (1+h); ans-1
%\end{Verbatim}
%Try various small values of $h$ and explain why $(1+h)-1$
%doesn't always give you $h$ back and what this has to do
%with roundoff error.
%\end{enumerate}

Finally, let's learn some wisdom about using finite difference
formulas on experimental data. Suppose you had acquired some data
that you needed to numerically differentiate. Since it's real data
there are random errors in the numbers.  Let's see how those errors
affect your ability to take numerical derivatives.\index{Data,
differentiating} \index{Differentiating data}
\begin{enumerate}
\prob \label{P:1.DerivExper} \marginfig{chapters/f01p7}{Plots of $f(x)$
    and $f'(x)$ from \ref{P:1.DerivExper} with 1000 points.
    $f''(x)$ has too much error to make a meaningful plot for
    this number of points.}

    Make a cell-edge grid for $0 \le x \le 5$ with $1000$ grid
    points. Then model some data with experimental errors in it
    by using Python's random number function {\tt rand} like
    this:
\begin{Verbatim}
f=cos(x)+.001*rand(1,length(x));
\end{Verbatim}
    So now $f$ contains the cosine function, plus experimental
    error at the 0.1\% level.  Calculate the first and second
    derivatives of this data and compare them to the ``real''
    derivatives (calculated without noise). Reduce the number of
    points to 100 and see what happens.
\end{enumerate}
Differentiating your data is a bad idea in general, and
differentiating it twice is even worse. If you can't avoid
differentiating experimental data, you had better work pretty
hard at reducing the error, or perhaps fit your data to a
smooth function, then differentiate the function.
